{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b14fc42-f583-4396-8dc1-d730b6ca3b0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apache Spark Project on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb187e7c-16bf-4efd-bdf3-7bf2ce4d53c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Uk5ghOBCxPlT"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73afcd50-40e2-4e13-808c-35039a0c9e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "61qYFUD1ysQX"
   },
   "source": [
    "There are datasets corresponding to a **list of health inspections in establishments** (restaurants, supermarkets, etc.), along with their respective health risks. Additionally, there is another dataset that shows a **description of these risks**.\n",
    "\n",
    "**The objective is to load these datasets under specific requirements and manipulate them according to the instructions of each exercise**.\n",
    "\n",
    "All necessary operations are described in the exercises, although extra tasks initiated by the student will be appreciated. The use of the DataFrame API will also be valued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a11d615-dcf1-4074-97a0-c07ea36e0d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8Z0h3dF9Vg4X"
   },
   "source": [
    "## Download Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18569743-aafa-4c9c-9807-fa07c494e912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Downloading two datasets directly from GitHub into the local Databricks driver node. The `--output-dir /databricks/driver` option specifies that the files will be downloaded into the `/databricks/driver` directory, which is accessible from Databricks notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13db421-61d6-43df-9aba-16a9d29d9997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/food_inspections_lite.csv' --output-dir /databricks/driver\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/risk_description.csv'  --output-dir /databricks/driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99582e7d-555b-4459-9b68-8bd62375b167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Copying the datasets from the local filesystem (`file:/databricks/driver/...`) into Databricks File System (`dbfs:`), making them accessible across the Spark cluster.\n",
    ">Note: DBFS is a distributed filesystem integrated with Databricks and ensures datasets are available to all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992d60-96b6-43cd-8e2d-9c52dd718662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp('file:/databricks/driver/food_inspections_lite.csv','dbfs:/dataset/food_inspections_lite.csv')\n",
    "dbutils.fs.cp('file:/databricks/driver/risk_description.csv','dbfs:/dataset/risk_description.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4970ce76-ee26-4eb4-8a55-51670bded867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Listing all files and directories under `/dataset/` on DBFS. It verifies that the previous copy operations succeeded, ensuring the files are correctly stored and accessible in DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29800e4-d003-4d6e-966d-0e98d8fff444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2fb42e4-f7b6-45d3-94fd-5301513d084e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Setting the Kafka bootstrap server address, which is used to connect to a Kafka cluster. This line defines a variable used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935edbd4-27be-48eb-ba38-8908a5ef3cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER=\"35.227.18.205:9094\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "991e0f26-28f7-4ed6-bc4d-596e759bf4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checkpointing allows Spark Structured Streaming to save its state (such as offsets, processed records, and metadata) periodically, enabling fault-tolerance and recovery from failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c984e88f-ddf1-4c44-ba66-5323887c62c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/tmp/project_spark/_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8dc933d-744b-40e5-aab6-0e18101a9c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- `spark.conf.set(...)` defines the location of the checkpoint files.\n",
    "- `spark.conf.get(...)` retrieves the checkpoint location to confirm that it has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4af6ee-1719-4495-9ae7-8c0444bdaedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", checkpoint_path)\n",
    "spark.conf.get(\"spark.sql.streaming.checkpointLocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57ec8fd-5255-4a35-b3a0-a1cfd42dae9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "prvVhMD4a5o7"
   },
   "source": [
    "## Exercise 1\n",
    "---\n",
    "1. **Create two dataframes, one from the file `food_inspections_lite.csv` and another from `risk_description.csv`**.\n",
    "2. **Convert these two dataframes into Delta tables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebfaafe1-d9e7-46d6-8127-bd71e79c37b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bu8hCWb8z-Pq",
    "outputId": "96556125-90b3-4e82-e20e-8ba1e50b55bf"
   },
   "outputs": [],
   "source": [
    "# Setting catalog and database\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"USE DATABASE default\")\n",
    "spark.sql(\"SELECT current_catalog(), current_database()\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f719b857-c609-45f2-9082-88cadc8ff1df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create DataFrames from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df8ae76-5c62-4c14-b389-3f0cfc6aed6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining file paths\n",
    "food_inspections_path = \"/dataset/food_inspections_lite.csv\"\n",
    "risk_description_path = \"/dataset/risk_description.csv\"\n",
    "\n",
    "# Loading CSV into DataFrames\n",
    "df_food_inspections = (spark.read\n",
    "                       .option(\"header\", True)\n",
    "                       .option(\"inferSchema\", True)                  \n",
    "                       .csv(food_inspections_path, header=True)\n",
    "                       )\n",
    "\n",
    "df_risk_description = (spark.read\n",
    "                       .option(\"header\", True)\n",
    "                       .option(\"inferSchema\", True)\n",
    "                       .csv(risk_description_path, header=True)\n",
    "                       )\n",
    "\n",
    "# Displaying schemas to verify\n",
    "df_food_inspections.printSchema()\n",
    "df_risk_description.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b80df81-8870-413e-ade1-4813863110c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dropping tables and paths before recreating them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd53fd0e-67fc-4a26-ae4d-bf464072257e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS hive_metastore.default.food_inspections\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS hive_metastore.default.risk_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc7efa4-81ac-40e5-a21b-9f480b1a7d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/food_inspections\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/risk_description\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a77108d-0ae9-499d-8e49-202eb74ddd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Convert DataFrames into Delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9035f6b-e65c-4500-8553-7945a49ff144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta tables don't allow column names containing spaces or certain special characters unless you explicitly enable column mapping.There are to options:\n",
    "* Rename columns to remove invalid characters\n",
    "* Enable Column Mapping (Delta table feature) that allows special characters and spaces in column names\n",
    "\n",
    "Applying option 1 as it is simpler and cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8925fe7-c167-4caa-ae51-e043a453fa4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Rename columns in df_food_inspections to remove spaces and special characters\n",
    "df_food_inspections_clean = df_food_inspections.select(\n",
    "    col(\"Inspection ID\").alias(\"inspection_id\"),\n",
    "    col(\"DBA Name\").alias(\"dba_name\"),\n",
    "    col(\"AKA Name\").alias(\"aka_name\"),\n",
    "    col(\"License #\").alias(\"license\"),\n",
    "    col(\"Facility Type\").alias(\"facility_type\"),\n",
    "    col(\"Risk\").alias(\"risk\"),\n",
    "    col(\"Address\").alias(\"address\"),\n",
    "    col(\"City\").alias(\"city\"),\n",
    "    col(\"State\").alias(\"state\"),\n",
    "    col(\"Zip\").alias(\"zip\"),\n",
    "    col(\"Inspection Date\").alias(\"inspection_date\"),\n",
    "    col(\"Inspection Type\").alias(\"inspection_type\"),\n",
    "    col(\"Results\").alias(\"results\"),\n",
    "    col(\"Violations\").alias(\"violations\"),\n",
    "    col(\"Latitude\").alias(\"latitude\"),\n",
    "    col(\"Longitude\").alias(\"longitude\"),\n",
    "    col(\"Location\").alias(\"location\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090e51d3-f20d-443c-9b3c-2ee457873f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving DataFrames as Delta tables\n",
    "df_food_inspections_clean.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .saveAsTable(\"food_inspections\")\n",
    "\n",
    "df_risk_description.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .saveAsTable(\"risk_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84a31f8-3c33-4667-ac86-aaea3bf6e2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verifying last changes\n",
    "spark.sql(\"DESCRIBE EXTENDED food_inspections\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbde9422-74d3-4277-ab41-7de958afc4a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Showing tables to verify\n",
    "spark.sql(\"SELECT * FROM food_inspections\").limit(5).display()\n",
    "spark.sql(\"SELECT * FROM risk_description\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d26822-a808-45a1-9f43-67a659891203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4-HOEezxVnCe"
   },
   "source": [
    "## Exercise 2\n",
    "---\n",
    "**Obtain the number of distinct inspections with High Risk `Risk 1 (High)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e919fcf-dcf8-4c39-95af-bd677ea39f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJ88z1pQV9WF",
    "outputId": "a99b4b66-0c0e-4476-d44e-3f8b3dc5f307"
   },
   "outputs": [],
   "source": [
    "high_risk_count = (\n",
    "    spark.table(\"food_inspections\")\n",
    "         .filter(col(\"risk\") == \"Risk 1 (High)\")\n",
    "         .select(\"inspection_id\")\n",
    "         .distinct()\n",
    "         .count()\n",
    ")\n",
    "\n",
    "print(f\"Distinct inspections with High Risk (Risk 1): {high_risk_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea37802e-9b95-49d0-a584-f24b7fda1b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3R1kpKmIXi-h"
   },
   "source": [
    "## Ejercicio 3\n",
    "---\n",
    "**From the previously loaded dataframes, obtain a table with the following columns:**\n",
    "1. `DBA Name`\n",
    "2. `Facility Type`\n",
    "3. `Risk`\n",
    "4. `Risk Description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f27c95d-eacc-4c91-8c93-eceb109145ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d382c2-4408-485f-81f7-dac4b6aa1466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crVCDIcYaFlu",
    "outputId": "b16795c6-4f5b-497f-a7ae-4cf565ac979a"
   },
   "outputs": [],
   "source": [
    "df_food = spark.table(\"food_inspections\")\n",
    "df_risk = spark.table(\"risk_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345c085d-a22e-4d1b-b6c7-0a1314c433ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare for joining\n",
    "Extracting the numeric `risk_id` from the `Risk` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763cdb67-68db-4b75-8769-84c75715a806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "df_food_prepared = df_food.withColumn(\n",
    "    \"risk_id\",\n",
    "    regexp_extract(col(\"risk\"), r\"Risk (\\d+)\", 1).cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0005545-0aef-450d-b17f-52706c8bc2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3527d767-6dff-46a7-b289-b34c88bcfa35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result = (\n",
    "    df_food_prepared\n",
    "    .join(df_risk, on=\"risk_id\", how=\"left\")\n",
    "    .select(\n",
    "        col(\"dba_name\").alias(\"DBA Name\"),\n",
    "        col(\"facility_type\").alias(\"Facility Type\"),\n",
    "        col(\"risk\").alias(\"Risk\"),\n",
    "        col(\"description\").alias(\"Risk Description\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee335d25-98c1-47c6-b3c1-828ca8db4b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result.limit(20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b089ed46-7d87-4edd-89f3-ad411ef5191c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZX1ahdYyb2L9"
   },
   "source": [
    "## Exercise 4\n",
    "---\n",
    "**Access the Spark UI to view the execution plan and describe each of the pieces/boxes that make up the execution plan. Add a screenshot of the analyzed execution plan.**\n",
    "\n",
    "> **Note:** A brief one-line description per box is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c475a932-d1eb-4bec-87e2-d82732320017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The image below shows the execution plan from Spark UI for the query joining `food_inspections_lite` and `risk_description`.\n",
    "\n",
    "![Execution Plan](images/Spark_physical_execution_plan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a5c29d-1447-4f97-838f-d3ab25e2bb3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The attached screenshot is a Spark physical execution plan (DAG) from the Spark UI (Query 36). Based on the image, this query performs a join between `food_inspections_lite` and `risk_description`, followed by some filters and projections.\n",
    "\n",
    "**WholeStageCodegen (Stage 39)**:\n",
    "Spark compiles and optimizes a part of the query into Java bytecode for efficient execution (used here for reading and filtering `risk_description`).\n",
    "\n",
    "**Filter (0)**:\n",
    "Filters records from the `risk_description` Delta table (possibly removing nulls or invalid entries).\n",
    "\n",
    "**ColumnarToRow (0)**:\n",
    "Converts columnar format data (optimized for storage) into row format for in-memory processing.\n",
    "\n",
    "**Scan parquet [hive_metastore.default.risk_description]**\n",
    "Reads the `risk_description` Delta table from storage in Parquet format.\n",
    "\n",
    "**BroadcastExchange (11)**:\n",
    "Broadcasts the small `risk_description` table to all worker nodes to speed up the join (broadcast join optimization).\n",
    "\n",
    "**WholeStageCodegen (Stage 38)**:\n",
    "Optimized compiled code for processing the larger table (`food_inspections_lite`) and performing the join.\n",
    "\n",
    "**Filter (0)**:\n",
    "Applies filters to `food_inspections_lite` (risk IS NOT NULL).\n",
    "\n",
    "**ColumnarToRow (0)**:\n",
    "Converts columnar data from Delta into row format for processing.\n",
    "\n",
    "**Scan parquet [hive_metastore.default.food_inspections_lite]**:\n",
    "Reads data from the `food_inspections_lite` Delta table.\n",
    "\n",
    "**Project (multiple layers)**:\n",
    "Selects only the necessary columns for output (e.g., DBA Name, Facility Type, Risk, Description).\n",
    "\n",
    "**LocalLimit and LimitLocalLimit and Limit**:\n",
    "Limits the number of output rows (a .show()).\n",
    "\n",
    "**BroadcastHashJoin (12)**:\n",
    "Performs the join between the two tables using a hash-based algorithm, optimized with broadcast. Broadcast joins avoid shuffling because:\n",
    "- The smaller table (`risk_description`) is broadcasted to all executors.\n",
    "- The larger table (`food_inspections_lite`) is scanned and joined locally on each node.\n",
    "\n",
    "**ResultQueryStage / AdaptiveSparkPlan**:\n",
    "Adaptive Spark Planning dynamically optimized the query plan based on actual data statistics during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff4369c-a922-4910-a85d-d4e0b7b28803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2sQNdQ2Gbz4p"
   },
   "source": [
    "## Exercise 5\n",
    "---\n",
    "1. **Obtain the number of inspections for each establishment (`DBA Name` column) and their result (`Results` column).**\n",
    "2. **Get the two establishments (`DBA Name`) with the most inspections for each of the results.**\n",
    "3. **Save the results in a new Delta table named `inspections_results`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "041174cd-9ec9-407b-9e35-b75e0914b51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Count inspections per establishment and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc5acf0-bda0-4605-9ead-f628ac9908f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLFgHfWp0pbL",
    "outputId": "0c1d2ade-e00c-4ffa-f1fd-ac650f7d4ad6"
   },
   "outputs": [],
   "source": [
    "df_food = spark.table(\"food_inspections\")\n",
    "\n",
    "# Count inspections by DBA Name and Results\n",
    "inspections_count_df = (\n",
    "    df_food.groupBy(\"dba_name\", \"results\")\n",
    "           .agg(count(\"*\")\n",
    "                .alias(\"inspection_count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b89e1b4-8420-4755-827d-d0ad993d2135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get the two establishments with most inspections per each result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71de34e1-036d-499f-8e90-5401f687648a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *\n",
    "\n",
    "# Defining window partitioned by Results, ordered by inspection_count descending\n",
    "window_spec = Window \\\n",
    "                .partitionBy(\"results\") \\\n",
    "                .orderBy(desc(\"inspection_count\"))\n",
    "\n",
    "# Adding row number per Results\n",
    "top_two_df = (\n",
    "    inspections_count_df\n",
    "        .withColumn(\"rank\", row_number().over(window_spec))\n",
    "        .filter(col(\"rank\") <= 2)\n",
    "        .select(\n",
    "          inspections_count_df[\"dba_name\"],\n",
    "          inspections_count_df[\"results\"],\n",
    "          inspections_count_df[\"inspection_count\"],\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b112150a-b4c9-424c-8852-0db214247bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save the results to a new Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d38ddbf-8ebd-46c3-b120-3127149d454b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS hive_metastore.default.inspections_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdaffcef-0e9f-41a6-b74e-322cf27bcfad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/user/hive/warehouse/inspections_results\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4303ee5-de2d-4d7d-9823-923f470a4ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_two_df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .saveAsTable(\"inspections_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f20f81f-d2f0-4ed6-af28-4c1b407fca6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verifying the results\n",
    "spark.table(\"inspections_results\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d86456-6a93-4b5d-aed1-dd2105921c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "04y2Ys6L0wTU"
   },
   "source": [
    "## Exercise 6\n",
    "---\n",
    "1. **Update the Delta table created in the previous exercise with the value `DBA_Name = \"error\"`.**\n",
    "2. **Restore the table to its original state.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8987f1d-b2cc-4ce8-b6b6-3fdfb430ae8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1Pp9VwU1DnK",
    "outputId": "b56672fa-5a19-4f6a-81de-522b931e433f"
   },
   "source": [
    "### Update Delta table (`inspections_results`) setting `dba_name` = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709c3e61-1cd6-4614-a045-69bde4755889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performing UPDATE\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE inspections_results\n",
    "    SET dba_name = 'error'\n",
    "\"\"\")\n",
    "\n",
    "# Verifying update\n",
    "spark.table(\"inspections_results\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30715fc9-1bae-44a3-b3f2-269c9f5440cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Restore the table to its original state using Delta Lake's Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f45986-5bcc-4f5b-b805-7e6c79c5518b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking Delta table history\n",
    "spark.sql(\"DESCRIBE HISTORY inspections_results\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1599cfe6-10d7-49e1-af58-f20240983981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restoring using the desired previous version (0)\n",
    "spark.sql(\"\"\"\n",
    "    RESTORE TABLE inspections_results TO VERSION AS OF 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2871bc47-f714-437d-8716-520819dcf3d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verifying restoration\n",
    "spark.table(\"inspections_results\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a86c2cf1-c6e3-47fa-8677-f2bc8699b5e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "77M6b7WwsTA1"
   },
   "source": [
    "## Exercise 7\n",
    "---\n",
    "**Create an application with Structured Streaming that reads data from the Kafka topic `inspections`.**\n",
    "> **Note:** The Kafka server URL is defined at the beginning of this notebook.\n",
    "\n",
    "**The data from this topic is exactly the same as the data being analyzed throughout this notebook, `Food Inspections`, so the schema is the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf81a580-fdc9-4808-bba5-eb25d465808f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set checkpoint path ensuring fault-tolerance and recovery\n",
    "checkpoint_path = \"/tmp/project_spark/_checkpoint\"\n",
    "\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", checkpoint_path)\n",
    "spark.conf.get(\"spark.sql.streaming.checkpointLocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58890d9d-8318-4263-ad99-7f46cd28c324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2313e8Du6RX",
    "outputId": "ed179851-dc6a-4ca8-ace7-144a407a3044"
   },
   "source": [
    "### Define the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63432768-6007-41a4-b572-34ad760f6f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When consuming data from Kafka using Structured Streaming, all fields in the Kafka message are initially interpreted as raw strings unless they are explicitly parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914dabac-2438-4c44-8b5a-2d6143cb77a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "food_inspections_schema = StructType([\n",
    "    StructField(\"Inspection ID\", StringType()),\n",
    "    StructField(\"DBA Name\", StringType()),\n",
    "    StructField(\"AKA Name\", StringType()),\n",
    "    StructField(\"License #\", StringType()),\n",
    "    StructField(\"Facility Type\", StringType()),\n",
    "    StructField(\"Risk\", StringType()),\n",
    "    StructField(\"Address\", StringType()),\n",
    "    StructField(\"City\", StringType()),\n",
    "    StructField(\"State\", StringType()),\n",
    "    StructField(\"Zip\", StringType()),\n",
    "    StructField(\"Inspection Date\", StringType()),  # will cast to DateType later\n",
    "    StructField(\"Inspection Type\", StringType()),\n",
    "    StructField(\"Results\", StringType()),\n",
    "    StructField(\"Violations\", StringType()),\n",
    "    StructField(\"Latitude\", StringType()),\n",
    "    StructField(\"Longitude\", StringType()),\n",
    "    StructField(\"Location\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "762e2e23-c897-4c0d-a95b-d8f76e4aeef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read from Kafka and parse JSON messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb5bc7a-a2cf-427f-ad14-d6835ab1764f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "kafka_stream = (\n",
    "    spark.readStream\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "         .option(\"subscribe\", \"inspections\")\n",
    "         .option(\"startingOffsets\", \"earliest\")\n",
    "         .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fba6ef0-dd2d-4829-830f-b226ee752af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cast fields to their correct types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "482346d6-d8b4-4dff-961e-488c3c4527bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For querying or joining, important fields can be casted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbb0715-9fc6-40cc-8ca7-8857efc03f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "cleaned_stream = kafka_stream \\\n",
    "        .selectExpr(\"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "        .withColumn(\"value\", from_json(\"value\", food_inspections_schema)) \\\n",
    "        .select(col('value.*'), col(\"timestamp\")) \\\n",
    "        .withColumn(\"Inspection ID\", col(\"Inspection ID\").cast(\"int\")) \\\n",
    "        .withColumn(\"License #\", col(\"License #\").cast(\"int\")) \\\n",
    "        .withColumn(\"Zip\", col(\"Zip\").cast(\"int\")) \\\n",
    "        .withColumn(\"Latitude\", col(\"Latitude\").cast(\"double\")) \\\n",
    "        .withColumn(\"Longitude\", col(\"Longitude\").cast(\"double\")) \\\n",
    "        .withColumn(\"Inspection Date\", to_date(col(\"Inspection Date\"), \"MM/dd/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd68cb4-c32c-4697-a86f-2258e914337b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_stream.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090c9d0b-305b-432c-a686-e87b3fca83be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JbyutPORhr0b"
   },
   "source": [
    "## Exercise 8\n",
    "---\n",
    "**Based on the data source from the previous exercise, obtain the number of inspections per `Facility Type` every 5 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5166960-90c6-4ea2-8c3f-31190d93f747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set checkpoint path ensuring fault-tolerance and recovery\n",
    "checkpoint_path = \"/tmp/project_spark/_checkpoint\"\n",
    "\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", checkpoint_path)\n",
    "spark.conf.get(\"spark.sql.streaming.checkpointLocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88c94ac9-f258-44ce-aaff-519f43e5dc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sparks receives records in micro-batches, JSON is parsed into a structured format, and columns like timestamp are preserved. Then Spark remembers the latest event time seen (timestamp) and drops records older than `max_event_time - 1 minute` (watermarking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996bdee1-cc39-492a-adac-0dbba743fb57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "facility_count = (\n",
    "    cleaned_stream\n",
    "        .withWatermark(\"timestamp\", \"1 minute\")\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"5 seconds\"),\n",
    "            col(\"Facility Type\")\n",
    "        )\n",
    "        .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268977fd-c122-4520-bea2-c2d40e329fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facility_count.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86fcae80-04aa-4d20-892b-b813877a2b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sr3vImmwxQiN"
   },
   "source": [
    "## Exercise 9\n",
    "---\n",
    "**Based on the data source from exercise 7, obtain the number of inspections by `Results` for the last 30 seconds every 5 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de705234-db33-4298-8b2a-89e1609edf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set checkpoint path ensuring fault-tolerance and recovery\n",
    "checkpoint_path = \"/tmp/project_spark/_checkpoint\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", checkpoint_path)\n",
    "spark.conf.get(\"spark.sql.streaming.checkpointLocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a1d8af9-03a0-467d-ace5-b56cb2dcc142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apply windowed aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3733881-6172-4d79-a935-3e6947e45036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_count = (\n",
    "    cleaned_stream\n",
    "        .withWatermark(\"timestamp\", \"1 minute\")\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"30 seconds\", \"5 seconds\"),\n",
    "            col(\"Results\")\n",
    "        )\n",
    "        .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd408c1d-53bc-440c-92a8-797baba7d610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_count.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14d4e4b5-b08f-41f9-a34b-7cf40f62b7c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BNea5uVAx1DG"
   },
   "source": [
    "## Exercise 10\n",
    "---\n",
    "1. **Update the Results column in the Delta table for food inspections created in Exercise 1 with the value `No result`.**\n",
    "2. **Now that the Delta table is corrupted with the value `No result`, the problem must be resolved with the data coming from Kafka, which will be assumed as the absolute truth. Therefore, it will need to be updated in real time as items arrive from Kafka**.\n",
    "> **Note**: It is recommended to stop all previous streams, as the one in this exercise tends to be resource-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "325badb0-c735-4f58-893b-ade49ee8eff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZ90YTcGsg0m",
    "outputId": "e28edd6d-9e6b-4695-c1ec-8f6d77b2b545"
   },
   "outputs": [],
   "source": [
    "# Set checkpoint path ensuring fault-tolerance and recovery\n",
    "checkpoint_path = \"/tmp/project_spark/_checkpoint\"\n",
    "\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", checkpoint_path)\n",
    "spark.conf.get(\"spark.sql.streaming.checkpointLocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce773b0-e093-4e8b-abb6-1e3993a30d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Simulate corruption on Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98fed89c-53b6-4769-801b-e561e3fa8ede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE food_inspections\n",
    "SET Results = 'No result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5845278-2015-4fcc-88ee-ff2b01a401d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT Inspection_ID, Results FROM food_inspections LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e3c3e3f-fae7-4666-a212-2972f8a0f2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the cleaned Kafka stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2477a8c8-196a-457c-8c13-9a16b1d9fc75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, to_date\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Schema\n",
    "food_inspections_schema = StructType([\n",
    "    StructField(\"Inspection ID\", StringType()),\n",
    "    StructField(\"DBA Name\", StringType()),\n",
    "    StructField(\"AKA Name\", StringType()),\n",
    "    StructField(\"License #\", StringType()),\n",
    "    StructField(\"Facility Type\", StringType()),\n",
    "    StructField(\"Risk\", StringType()),\n",
    "    StructField(\"Address\", StringType()),\n",
    "    StructField(\"City\", StringType()),\n",
    "    StructField(\"State\", StringType()),\n",
    "    StructField(\"Zip\", StringType()),\n",
    "    StructField(\"Inspection Date\", StringType()),\n",
    "    StructField(\"Inspection Type\", StringType()),\n",
    "    StructField(\"Results\", StringType()),\n",
    "    StructField(\"Violations\", StringType()),\n",
    "    StructField(\"Latitude\", StringType()),\n",
    "    StructField(\"Longitude\", StringType()),\n",
    "    StructField(\"Location\", StringType())\n",
    "])\n",
    "\n",
    "kafka_stream = (\n",
    "    spark.readStream\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "         .option(\"subscribe\", \"inspections\")\n",
    "         .option(\"startingOffsets\", \"latest\")\n",
    "         .load()\n",
    ")\n",
    "\n",
    "cleaned_stream = (\n",
    "    kafka_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "                .select(from_json(col(\"value\"), food_inspections_schema).alias(\"data\"))\n",
    "                .select(\"data.*\")\n",
    "                .withColumn(\"Inspection ID\", col(\"Inspection ID\").cast(\"int\"))\n",
    "                .withColumn(\"License #\", col(\"License #\").cast(\"int\"))\n",
    "                .withColumn(\"Zip\", col(\"Zip\").cast(\"int\"))\n",
    "                .withColumn(\"Latitude\", col(\"Latitude\").cast(\"double\"))\n",
    "                .withColumn(\"Longitude\", col(\"Longitude\").cast(\"double\"))\n",
    "                .withColumn(\"Inspection Date\", to_date(col(\"Inspection Date\"), \"MM/dd/yyyy\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4f14623-f18b-45e1-947d-dcfd5a9056a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define foreachBatch upsert function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9add6254-0a0d-489c-b446-fa9b18ecc4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_inspections_from_kafka(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    delta_path = \"/delta/food_inspections\"\n",
    "\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "        (\n",
    "            delta_table.alias(\"t\")\n",
    "                .merge(\n",
    "                    batch_df.alias(\"s\"),\n",
    "                    \"t.`Inspection ID` = s.`Inspection ID`\"\n",
    "                )\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Table not found, skipping upsert in batch {batch_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c27a919c-6690-4239-94e6-ee6148121cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Start the streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b8e200-5ee2-4ac2-a986-512110aa916a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    cleaned_stream.writeStream\n",
    "        .foreachBatch(upsert_inspections_from_kafka)\n",
    "        .option(\"checkpointLocation\", \"/tmp/food_inspections_checkpoint\")\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8219f113-6ac8-4085-a02f-2eedbcdbf0df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify the fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d4dd2c-2e4b-41a1-b4f8-59e2330e12b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT `inspection_id`, `Results`\n",
    "FROM food_inspections\n",
    "WHERE `Results` != 'No result'\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "659b713d-9f62-4cf6-9276-d85a8db7329e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 11\n",
    "---\n",
    "**Design a real-time analysis solution using Apache Spark in Databricks to consume flight data transmitted by Kafka. These data should be stored in a Delta table, and the current position of the flights should be visualized on a map.**\n",
    "* **Flight data is in a topic called `flights`.**\n",
    "* **Save all the flights in a Delta table, but only one entry per flight code, so if updates on the flight position are received, the corresponding record will be updated. This must happen in real-time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e4d8326-c846-4db8-8fa6-942f29fc7f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **Note**: For more information on the input data, refer to [OpenSky Network](https://openskynetwork.github.io/opensky-api/rest.html#all-state-vectors). A screenshot of the visualization to be achieved is shown below. Keep in mind that this map visualization is available in Databricks, so there will be no need to import any external libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47cdf5d1-fda5-4464-9f8c-0a3dce006e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Flight Map](https://raw.githubusercontent.com/masfworld/datahack_docker/ab487794745499248388b67cf574085c5d86746e/zeppelin/data/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a217ef44-1693-4b8f-9a0f-ee509d812991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/tmp/project_spark/_checkpoint\"\n",
    "\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", checkpoint_path)\n",
    "spark.conf.get(\"spark.sql.streaming.checkpointLocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eef3692-e59d-41f7-bc14-58f19a1efd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a67b43-3833-4e62-95e9-6d6426579ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = \"35.227.18.205:9094\"\n",
    "KAFKA_TOPIC = \"flights\"\n",
    "DELTA_FLIGHTS_PATH = \"/delta/flights\"\n",
    "CHECKPOINT_PATH = \"/tmp/flights_checkpoint\"\n",
    "\n",
    "flights_schema = StructType([\n",
    "    StructField(\"icao24\", StringType(), True),\n",
    "    StructField(\"callsign\", StringType(), True),\n",
    "    StructField(\"origin_country\", StringType(), True),\n",
    "    StructField(\"time_position\", LongType(), True),\n",
    "    StructField(\"last_contact\", LongType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"on_ground\", BooleanType(), True),\n",
    "    StructField(\"velocity\", DoubleType(), True),\n",
    "    StructField(\"heading\", DoubleType(), True),\n",
    "    StructField(\"squawk\", StringType(), True),\n",
    "    StructField(\"spi\", BooleanType(), True),\n",
    "    StructField(\"position_source\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9555c7-3004-4ed6-be55-b07e539b41e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Consume and parse streaming data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c78813-2aae-4ca4-9f98-aaedafd6c472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime\n",
    "\n",
    "raw_flights_stream = (\n",
    "    spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "        .option(\"subscribe\", KAFKA_TOPIC)\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .load()\n",
    ")\n",
    "\n",
    "parsed_flights = (\n",
    "    raw_flights_stream.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "                      .select(from_json(col(\"json\"), flight_schema).alias(\"data\"))\n",
    "                      .select(\"data.*\")\n",
    "                      .withColumn(\"event_time\", from_unixtime(col(\"last_contact\")).cast(\"timestamp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee0d4bf-838d-4bd4-8e03-86d78a14fd38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS flights;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bc754b-3fb7-4045-bb72-ad080ba5541a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FLIGHTS_TABLE_PATH = \"/user/hive/warehouse/flights\"\n",
    "dbutils.fs.rm(FLIGHTS_TABLE_PATH, recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5cad886-f777-4b2e-93b5-5462c4612377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Real-time upsert into a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9eae230-ff6e-44a7-a004-ef410cb5b977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_flights(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    delta_path = DELTA_FLIGHTS_PATH\n",
    "\n",
    "    if DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "        (\n",
    "            delta_table.alias(\"t\")\n",
    "                .merge(\n",
    "                    batch_df.alias(\"s\"),\n",
    "                    \"t.icao24 = s.icao24\"\n",
    "                )\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "        )\n",
    "    else:\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS flights USING DELTA LOCATION '{delta_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c0ce134-4722-4c50-98cd-ae0801d8094c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Streaming query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc07bba1-c7ed-4116-b21e-fc8ec9e014bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    parsed_flights.writeStream\n",
    "        .foreachBatch(upsert_flights)\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af288e4-b318-414d-a4e6-8448982debfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visualize flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "092147d6-54a8-40df-932d-ea0e464c139e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT latitude, longitude\n",
    "FROM flights\n",
    "WHERE latitude IS NOT NULL AND longitude IS NOT NULL"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8527763804301552,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Apache_Spark_Project_en",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "Jq9d0x1OTh2N",
    "8Z0h3dF9Vg4X"
   ],
   "name": "Evaluacion_Apache_Spark_Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
